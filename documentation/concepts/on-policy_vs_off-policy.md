[lucidium](https://github.com/theokoles7/lucidium) / [documentation](https://github.com/theokoles7/lucidium/blob/main/documentation/README.md) / [concepts](./README.md) / on-policy_vs_off-policy

# On-Policy vs. Off-Policy Learning in Reinforcement Learning

## Overview

This document explores the fundamental distinction between **On-Policy** and **Off-Policy** learning paradigms in the context of **Reinforcement Learning (RL)**. Understanding this difference is crucial for designing effective RL algorithms that can learn optimal policies in various environments.

## On-Policy Learning

**On-Policy Learning** refers to methods where the agent learns about the policy it is currently following. In this paradigm, the agent's actions are directly influenced by its current policy, and it updates this policy based on the rewards received from those actions.

### Characteristics:
- **Policy Evaluation**: The agent evaluates the current policy by collecting data from its own actions.
- **Policy Improvement**: The agent updates its policy based on the rewards received while following that policy.
- **Exploration vs. Exploitation**: On-policy methods often balance exploration (trying new actions) and exploitation (choosing the best-known actions) within the same policy.

### Examples:
- **SARSA (State-Action-Reward-State-Action)**: An on-policy algorithm that updates the action-value function based on the action taken by the current policy.
- **Policy Gradient Methods**: These methods directly optimize the policy by adjusting its parameters based on the gradient of expected rewards

### Pros:
- Can lead to more stable learning since the policy is updated based on actions taken by the agent.
- Often easier to implement and understand due to the direct relationship between policy and learning.

### Cons:
- May converge to suboptimal policies if the exploration is not sufficient.
- Requires more data to learn effectively since it relies on the current policy for both exploration and exploitation

## Off-Policy Learning

**Off-Policy Learning** refers to methods where the agent learns about a policy different from the one it is currently following. In this paradigm, the agent can learn from actions taken by other policies or even from past experiences.

### Characteristics:
- **Policy Evaluation**: The agent can evaluate multiple policies simultaneously, including those not currently being followed.
- **Policy Improvement**: The agent can update its policy based on data collected from different policies or from a replay buffer.
- **Exploration vs. Exploitation**: Off-policy methods can separate exploration and exploitation, allowing for more efficient learning.

### Examples:
- **Q-Learning**: An off-policy algorithm that learns the optimal action-value function independently of the policy being followed.
- **Deep Q-Networks (DQN)**: Combines Q-learning with deep neural networks and uses experience replay to learn from past experiences.

### Pros:
- Can learn from a wider range of experiences, including those generated by different policies.
- More sample-efficient since it can reuse past experiences.
- Can learn optimal policies even when the agent is exploring suboptimal actions.

### Cons:
- More complex to implement due to the need for mechanisms like experience replay.
- Can suffer from instability and divergence if not properly managed, especially in function approximation settings.